{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1 COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nâœ… COMPLETED TASKS:\")\n",
    "print(f\"   1. âœ… Loaded dataset from Student_At_Risk_Student_Data.csv\")\n",
    "print(f\"   2. âœ… Verified 'risk' outcome variable exists\")\n",
    "print(f\"   3. âœ… Reorganized columns: 'risk' at end, 'country' at 2nd position\")\n",
    "print(f\"   4. âœ… Converted all text data to lowercase\")\n",
    "print(f\"   5. âœ… Filtered for 'medium' and 'high' risk students only\")\n",
    "print(f\"   6. âœ… Saved processed data as 'engineered_student_data.csv'\")\n",
    "print(f\"   7. âœ… Explored data patterns for synthetic generation preparation\")\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL DATASET STATUS:\")\n",
    "print(f\"   â€¢ File: data/refined_data_for_model/engineered_student_data.csv\")\n",
    "print(f\"   â€¢ Shape: {df.shape}\")\n",
    "print(f\"   â€¢ Risk levels: {list(df['risk'].unique())}\")\n",
    "print(f\"   â€¢ Ready for Phase 2 feature engineering\")\n",
    "\n",
    "print(f\"\\nðŸš€ NEXT STEPS (Phase 2):\")\n",
    "print(f\"   â€¢ Correlation analysis with outcome variable\")\n",
    "print(f\"   â€¢ Create JSON mapping for categorical variables\")\n",
    "print(f\"   â€¢ Assign numeric weights based on correlations\")\n",
    "print(f\"   â€¢ Sentiment analysis for comments\")\n",
    "print(f\"   â€¢ NLP processing for text features\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1 Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data patterns for synthetic data generation preparation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA EXPLORATION FOR SYNTHETIC DATA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic dataset overview\n",
    "print(f\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Features: {df.shape[1] - 1} (excluding target)\")\n",
    "print(f\"   Target: 'risk' column\")\n",
    "\n",
    "# Risk distribution\n",
    "print(f\"\\n2. TARGET DISTRIBUTION:\")\n",
    "risk_counts = df['risk'].value_counts()\n",
    "risk_percentages = df['risk'].value_counts(normalize=True) * 100\n",
    "for risk_level in risk_counts.index:\n",
    "    print(f\"   {risk_level}: {risk_counts[risk_level]} students ({risk_percentages[risk_level]:.1f}%)\")\n",
    "\n",
    "# Data types overview\n",
    "print(f\"\\n3. FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"   Numeric features: {len(numeric_cols)} columns\")\n",
    "print(f\"   Text features: {len(text_cols)} columns\")\n",
    "\n",
    "# Missing data analysis\n",
    "print(f\"\\n4. MISSING DATA ANALYSIS:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"   Columns with missing data:\")\n",
    "    for col, count in missing_data.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"     {col}: {count} missing ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"   No missing data found\")\n",
    "\n",
    "# Key categorical features\n",
    "print(f\"\\n5. KEY CATEGORICAL FEATURES:\")\n",
    "categorical_features = ['country', 'course', 'academic_status', 'student_cohort']\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        unique_count = df[feature].nunique()\n",
    "        print(f\"   {feature}: {unique_count} unique values\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"     Values: {list(df[feature].unique())}\")\n",
    "\n",
    "print(f\"\\n6. NUMERIC FEATURES SUMMARY:\")\n",
    "if len(numeric_cols) > 1:  # Exclude student_id\n",
    "    numeric_features = [col for col in numeric_cols if col != 'student_id']\n",
    "    print(f\"   Assessment scores: {len([col for col in numeric_features if 'assess' in col])} columns\")\n",
    "    print(f\"   Attendance features: {len([col for col in numeric_features if 'attendance' in col])} columns\")\n",
    "    print(f\"   Other numeric: {len([col for col in numeric_features if 'assess' not in col and 'attendance' not in col])} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Data Exploration for Synthetic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data as engineered_student_data.csv\n",
    "output_path = '../data/refined_data_for_model/engineered_student_data.csv'\n",
    "\n",
    "print(f\"Saving processed data to: {output_path}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Final columns: {list(df.columns)}\")\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nData successfully saved!\")\n",
    "print(f\"File location: {output_path}\")\n",
    "\n",
    "# Verify the saved file\n",
    "verification_df = pd.read_csv(output_path)\n",
    "print(f\"\\nVerification - loaded file shape: {verification_df.shape}\")\n",
    "print(f\"Verification - risk distribution:\")\n",
    "print(verification_df['risk'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only 'medium' and 'high' risk students\n",
    "print(\"Current risk distribution:\")\n",
    "print(df['risk'].value_counts())\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "\n",
    "# Filter for medium and high risk students only\n",
    "df_filtered = df[df['risk'].isin(['medium', 'high'])].copy()\n",
    "\n",
    "print(f\"Filtered dataset shape: {df_filtered.shape}\")\n",
    "print(f\"\\nFiltered risk distribution:\")\n",
    "print(df_filtered['risk'].value_counts())\n",
    "\n",
    "# Update the main dataframe\n",
    "df = df_filtered\n",
    "\n",
    "print(f\"\\nFinal dataset shape after filtering: {df.shape}\")\n",
    "print(f\"Students removed: {698 - df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter for Medium and High Risk Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text data to lowercase\n",
    "print(\"Converting all text data to lowercase...\")\n",
    "\n",
    "# Identify object (text) columns\n",
    "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Text columns to convert: {text_columns}\")\n",
    "\n",
    "# Convert all text columns to lowercase\n",
    "for col in text_columns:\n",
    "    if col != 'student_id':  # Skip student_id if it's text\n",
    "        df[col] = df[col].astype(str).str.lower()\n",
    "\n",
    "print(\"Text data conversion completed!\")\n",
    "\n",
    "# Display sample of converted data\n",
    "print(f\"\\nSample of converted data:\")\n",
    "print(df[text_columns[:5]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert Text Data to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorganize columns - move 'risk' to end, 'country' to 2nd position after 'student_id'\n",
    "print(\"Original column order:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "# Get current column list\n",
    "columns = list(df.columns)\n",
    "\n",
    "# Remove 'risk' and 'country' from their current positions\n",
    "columns.remove('risk')\n",
    "columns.remove('country')\n",
    "\n",
    "# Create new column order: student_id, country, other columns, risk\n",
    "new_columns = ['student_id', 'country'] + [col for col in columns if col != 'student_id'] + ['risk']\n",
    "\n",
    "# Reorder the dataframe\n",
    "df = df[new_columns]\n",
    "\n",
    "print(f\"\\nNew column order:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nDataset shape after reordering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reorganize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'risk' column values and verify it exists\n",
    "print(\"Risk column found!\")\n",
    "print(f\"Risk column values: {df['risk'].unique()}\")\n",
    "print(f\"Risk column value counts:\")\n",
    "print(df['risk'].value_counts())\n",
    "\n",
    "# Check if 'country' column exists\n",
    "if 'country' in df.columns:\n",
    "    print(f\"\\nCountry column found!\")\n",
    "    print(f\"Country column unique count: {df['country'].nunique()}\")\n",
    "else:\n",
    "    print(\"Country column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Verify Risk and Country Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Phase 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the refined dataset\n",
    "df = pd.read_csv('../data/refined_data_for_model/Student_At_Risk_Student_Data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for Student Risk Prediction\n",
    "\n",
    "This notebook focuses on feature engineering techniques to create meaningful features for predicting students at high and medium risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
