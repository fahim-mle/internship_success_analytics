{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "# Feature Engineering for Student Risk Prediction\n",
    "\n",
    "This notebook focuses on feature engineering techniques to create meaningful features for predicting students at high and medium risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (698, 41)\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 698 entries, 0 to 697\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   student_id                    698 non-null    int64  \n",
      " 1   course                        698 non-null    object \n",
      " 2   student_cohort                698 non-null    object \n",
      " 3   academic_status               698 non-null    object \n",
      " 4   failed_subjects               698 non-null    int64  \n",
      " 5   study_skills(attended)        698 non-null    object \n",
      " 6   referral                      698 non-null    object \n",
      " 7   pp_meeting                    698 non-null    object \n",
      " 8   self_assessment               109 non-null    object \n",
      " 9   readiness_assessment_results  698 non-null    object \n",
      " 10  follow_up                     698 non-null    object \n",
      " 11  follow_up_type                698 non-null    object \n",
      " 12  subject_1                     698 non-null    object \n",
      " 13  subject_1_assess_1            698 non-null    float64\n",
      " 14  subject_1_assess_2            698 non-null    float64\n",
      " 15  subject_1_assess_3            698 non-null    float64\n",
      " 16  subject_1_assess_4            698 non-null    float64\n",
      " 17  attendance_1                  698 non-null    float64\n",
      " 18  learn_jcu_issues_1            698 non-null    object \n",
      " 19  lecturer_referral_1           698 non-null    object \n",
      " 20  subject_2                     698 non-null    object \n",
      " 21  subject_2_assess_1            698 non-null    float64\n",
      " 22  subject_2_assess_2            698 non-null    float64\n",
      " 23  subject_2_assess_3            698 non-null    float64\n",
      " 24  subject_2_assess_4            698 non-null    float64\n",
      " 25  attendance_2                  698 non-null    float64\n",
      " 26  learn_jcu_issues_2            698 non-null    object \n",
      " 27  lecturer_referral_2           698 non-null    object \n",
      " 28  subject_3                     698 non-null    object \n",
      " 29  subject_3_assess_1            698 non-null    float64\n",
      " 30  subject_3_assess_2            698 non-null    float64\n",
      " 31  subject_3_assess_3            698 non-null    float64\n",
      " 32  subject_3_assess_4            698 non-null    float64\n",
      " 33  attendance_3                  698 non-null    float64\n",
      " 34  learn_jcu_issues_3            698 non-null    object \n",
      " 35  lecturer_referral_3           698 non-null    object \n",
      " 36  comments                      239 non-null    object \n",
      " 37  identified_issues             698 non-null    object \n",
      " 38  course_group                  698 non-null    object \n",
      " 39  risk                          698 non-null    object \n",
      " 40  country                       698 non-null    object \n",
      "dtypes: float64(15), int64(2), object(24)\n",
      "memory usage: 223.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the refined dataset\n",
    "df = pd.read_csv('../data/refined_data_for_model/Student_At_Risk_Student_Data.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 2. Phase 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Step 1: Verify Risk and Country Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk column found!\n",
      "Risk column values: ['High' 'Low' 'Medium']\n",
      "Risk column value counts:\n",
      "risk\n",
      "Low       416\n",
      "Medium    187\n",
      "High       95\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Country column found!\n",
      "Country column unique count: 43\n"
     ]
    }
   ],
   "source": [
    "# Check the 'risk' column values and verify it exists\n",
    "print(\"Risk column found!\")\n",
    "print(f\"Risk column values: {df['risk'].unique()}\")\n",
    "print(f\"Risk column value counts:\")\n",
    "print(df['risk'].value_counts())\n",
    "\n",
    "# Check if 'country' column exists\n",
    "if 'country' in df.columns:\n",
    "    print(f\"\\nCountry column found!\")\n",
    "    print(f\"Country column unique count: {df['country'].nunique()}\")\n",
    "else:\n",
    "    print(\"Country column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Step 2: Reorganize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column order:\n",
      "['student_id', 'course', 'student_cohort', 'academic_status', 'failed_subjects', 'study_skills(attended)', 'referral', 'pp_meeting', 'self_assessment', 'readiness_assessment_results', 'follow_up', 'follow_up_type', 'subject_1', 'subject_1_assess_1', 'subject_1_assess_2', 'subject_1_assess_3', 'subject_1_assess_4', 'attendance_1', 'learn_jcu_issues_1', 'lecturer_referral_1', 'subject_2', 'subject_2_assess_1', 'subject_2_assess_2', 'subject_2_assess_3', 'subject_2_assess_4', 'attendance_2', 'learn_jcu_issues_2', 'lecturer_referral_2', 'subject_3', 'subject_3_assess_1', 'subject_3_assess_2', 'subject_3_assess_3', 'subject_3_assess_4', 'attendance_3', 'learn_jcu_issues_3', 'lecturer_referral_3', 'comments', 'identified_issues', 'course_group', 'risk', 'country']\n",
      "\n",
      "New column order:\n",
      "['student_id', 'country', 'course', 'student_cohort', 'academic_status', 'failed_subjects', 'study_skills(attended)', 'referral', 'pp_meeting', 'self_assessment', 'readiness_assessment_results', 'follow_up', 'follow_up_type', 'subject_1', 'subject_1_assess_1', 'subject_1_assess_2', 'subject_1_assess_3', 'subject_1_assess_4', 'attendance_1', 'learn_jcu_issues_1', 'lecturer_referral_1', 'subject_2', 'subject_2_assess_1', 'subject_2_assess_2', 'subject_2_assess_3', 'subject_2_assess_4', 'attendance_2', 'learn_jcu_issues_2', 'lecturer_referral_2', 'subject_3', 'subject_3_assess_1', 'subject_3_assess_2', 'subject_3_assess_3', 'subject_3_assess_4', 'attendance_3', 'learn_jcu_issues_3', 'lecturer_referral_3', 'comments', 'identified_issues', 'course_group', 'risk']\n",
      "\n",
      "Dataset shape after reordering: (698, 41)\n"
     ]
    }
   ],
   "source": [
    "# Reorganize columns - move 'risk' to end, 'country' to 2nd position after 'student_id'\n",
    "print(\"Original column order:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "# Get current column list\n",
    "columns = list(df.columns)\n",
    "\n",
    "# Remove 'risk' and 'country' from their current positions\n",
    "columns.remove('risk')\n",
    "columns.remove('country')\n",
    "\n",
    "# Create new column order: student_id, country, other columns, risk\n",
    "new_columns = ['student_id', 'country'] + [col for col in columns if col != 'student_id'] + ['risk']\n",
    "\n",
    "# Reorder the dataframe\n",
    "df = df[new_columns]\n",
    "\n",
    "print(f\"\\nNew column order:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nDataset shape after reordering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Step 3: Convert Text Data to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting all text data to lowercase...\n",
      "Text columns to convert: ['country', 'course', 'student_cohort', 'academic_status', 'study_skills(attended)', 'referral', 'pp_meeting', 'self_assessment', 'readiness_assessment_results', 'follow_up', 'follow_up_type', 'subject_1', 'learn_jcu_issues_1', 'lecturer_referral_1', 'subject_2', 'learn_jcu_issues_2', 'lecturer_referral_2', 'subject_3', 'learn_jcu_issues_3', 'lecturer_referral_3', 'comments', 'identified_issues', 'course_group', 'risk']\n",
      "Text data conversion completed!\n",
      "\n",
      "Sample of converted data:\n",
      "      country course   student_cohort   academic_status  \\\n",
      "0   australia    mba       continuing       conditional   \n",
      "1   australia    mba      transferred      satisfactory   \n",
      "2  bangladesh    mba              new      satisfactory   \n",
      "3      bhutan    mba      sri to jcub      satisfactory   \n",
      "4      bhutan    mba  return to study  academic caution   \n",
      "\n",
      "         study_skills(attended)  \n",
      "0              essential skills  \n",
      "1                   referencing  \n",
      "2                       writing  \n",
      "3              essential skills  \n",
      "4  essential skills and reading  \n"
     ]
    }
   ],
   "source": [
    "# Convert all text data to lowercase\n",
    "print(\"Converting all text data to lowercase...\")\n",
    "\n",
    "# Identify object (text) columns\n",
    "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Text columns to convert: {text_columns}\")\n",
    "\n",
    "# Convert all text columns to lowercase\n",
    "for col in text_columns:\n",
    "    if col != 'student_id':  # Skip student_id if it's text\n",
    "        df[col] = df[col].astype(str).str.lower()\n",
    "\n",
    "print(\"Text data conversion completed!\")\n",
    "\n",
    "# Display sample of converted data\n",
    "print(f\"\\nSample of converted data:\")\n",
    "print(df[text_columns[:5]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Step 4: Filter for Medium and High Risk Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current risk distribution:\n",
      "risk\n",
      "low       416\n",
      "medium    187\n",
      "high       95\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original dataset shape: (698, 41)\n",
      "Filtered dataset shape: (282, 41)\n",
      "\n",
      "Filtered risk distribution:\n",
      "risk\n",
      "medium    187\n",
      "high       95\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final dataset shape after filtering: (282, 41)\n",
      "Students removed: 416\n"
     ]
    }
   ],
   "source": [
    "# Filter for only 'medium' and 'high' risk students\n",
    "print(\"Current risk distribution:\")\n",
    "print(df['risk'].value_counts())\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "\n",
    "# Filter for medium and high risk students only\n",
    "df_filtered = df[df['risk'].isin(['medium', 'high'])].copy()\n",
    "\n",
    "print(f\"Filtered dataset shape: {df_filtered.shape}\")\n",
    "print(f\"\\nFiltered risk distribution:\")\n",
    "print(df_filtered['risk'].value_counts())\n",
    "\n",
    "# Update the main dataframe\n",
    "df = df_filtered\n",
    "\n",
    "print(f\"\\nFinal dataset shape after filtering: {df.shape}\")\n",
    "print(f\"Students removed: {698 - df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Step 5: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to: ../data/refined_data_for_model/engineered_student_data.csv\n",
      "Final dataset shape: (282, 41)\n",
      "Final columns: ['student_id', 'country', 'course', 'student_cohort', 'academic_status', 'failed_subjects', 'study_skills(attended)', 'referral', 'pp_meeting', 'self_assessment', 'readiness_assessment_results', 'follow_up', 'follow_up_type', 'subject_1', 'subject_1_assess_1', 'subject_1_assess_2', 'subject_1_assess_3', 'subject_1_assess_4', 'attendance_1', 'learn_jcu_issues_1', 'lecturer_referral_1', 'subject_2', 'subject_2_assess_1', 'subject_2_assess_2', 'subject_2_assess_3', 'subject_2_assess_4', 'attendance_2', 'learn_jcu_issues_2', 'lecturer_referral_2', 'subject_3', 'subject_3_assess_1', 'subject_3_assess_2', 'subject_3_assess_3', 'subject_3_assess_4', 'attendance_3', 'learn_jcu_issues_3', 'lecturer_referral_3', 'comments', 'identified_issues', 'course_group', 'risk']\n",
      "\n",
      "Data successfully saved!\n",
      "File location: ../data/refined_data_for_model/engineered_student_data.csv\n",
      "\n",
      "Verification - loaded file shape: (282, 41)\n",
      "Verification - risk distribution:\n",
      "risk\n",
      "medium    187\n",
      "high       95\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save processed data as engineered_student_data.csv\n",
    "output_path = '../data/refined_data_for_model/engineered_student_data.csv'\n",
    "\n",
    "print(f\"Saving processed data to: {output_path}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Final columns: {list(df.columns)}\")\n",
    "\n",
    "# Save the processed dataset\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nData successfully saved!\")\n",
    "print(f\"File location: {output_path}\")\n",
    "\n",
    "# Verify the saved file\n",
    "verification_df = pd.read_csv(output_path)\n",
    "print(f\"\\nVerification - loaded file shape: {verification_df.shape}\")\n",
    "print(f\"Verification - risk distribution:\")\n",
    "print(verification_df['risk'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Step 6: Data Exploration for Synthetic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA EXPLORATION FOR SYNTHETIC DATA GENERATION\n",
      "============================================================\n",
      "\n",
      "1. DATASET OVERVIEW:\n",
      "   Shape: (282, 41)\n",
      "   Features: 40 (excluding target)\n",
      "   Target: 'risk' column\n",
      "\n",
      "2. TARGET DISTRIBUTION:\n",
      "   medium: 187 students (66.3%)\n",
      "   high: 95 students (33.7%)\n",
      "\n",
      "3. FEATURE TYPES:\n",
      "   Numeric features: 17 columns\n",
      "   Text features: 24 columns\n",
      "\n",
      "4. MISSING DATA ANALYSIS:\n",
      "   No missing data found\n",
      "\n",
      "5. KEY CATEGORICAL FEATURES:\n",
      "   country: 32 unique values\n",
      "   course: 14 unique values\n",
      "   academic_status: 4 unique values\n",
      "     Values: ['conditional', 'satisfactory', 'academic caution', 'excluded']\n",
      "   student_cohort: 8 unique values\n",
      "     Values: ['continuing', 'new', 'return to study', 'loa', 'first year', 'transferred', 'excluded', 'sri to jcub']\n",
      "\n",
      "6. NUMERIC FEATURES SUMMARY:\n",
      "   Assessment scores: 12 columns\n",
      "   Attendance features: 3 columns\n",
      "   Other numeric: 1 columns\n"
     ]
    }
   ],
   "source": [
    "# Explore data patterns for synthetic data generation preparation\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA EXPLORATION FOR SYNTHETIC DATA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic dataset overview\n",
    "print(f\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Features: {df.shape[1] - 1} (excluding target)\")\n",
    "print(f\"   Target: 'risk' column\")\n",
    "\n",
    "# Risk distribution\n",
    "print(f\"\\n2. TARGET DISTRIBUTION:\")\n",
    "risk_counts = df['risk'].value_counts()\n",
    "risk_percentages = df['risk'].value_counts(normalize=True) * 100\n",
    "for risk_level in risk_counts.index:\n",
    "    print(f\"   {risk_level}: {risk_counts[risk_level]} students ({risk_percentages[risk_level]:.1f}%)\")\n",
    "\n",
    "# Data types overview\n",
    "print(f\"\\n3. FEATURE TYPES:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "text_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"   Numeric features: {len(numeric_cols)} columns\")\n",
    "print(f\"   Text features: {len(text_cols)} columns\")\n",
    "\n",
    "# Missing data analysis\n",
    "print(f\"\\n4. MISSING DATA ANALYSIS:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"   Columns with missing data:\")\n",
    "    for col, count in missing_data.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"     {col}: {count} missing ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"   No missing data found\")\n",
    "\n",
    "# Key categorical features\n",
    "print(f\"\\n5. KEY CATEGORICAL FEATURES:\")\n",
    "categorical_features = ['country', 'course', 'academic_status', 'student_cohort']\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        unique_count = df[feature].nunique()\n",
    "        print(f\"   {feature}: {unique_count} unique values\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"     Values: {list(df[feature].unique())}\")\n",
    "\n",
    "print(f\"\\n6. NUMERIC FEATURES SUMMARY:\")\n",
    "if len(numeric_cols) > 1:  # Exclude student_id\n",
    "    numeric_features = [col for col in numeric_cols if col != 'student_id']\n",
    "    print(f\"   Assessment scores: {len([col for col in numeric_features if 'assess' in col])} columns\")\n",
    "    print(f\"   Attendance features: {len([col for col in numeric_features if 'attendance' in col])} columns\")\n",
    "    print(f\"   Other numeric: {len([col for col in numeric_features if 'assess' not in col and 'attendance' not in col])} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 3. Phase 1 Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(\"\\n\" + \"=\" * 60)\\nprint(\"PHASE 1 COMPLETION SUMMARY\")\\nprint(\"=\" * 60)\\n\\nprint(f\"\\n✅ COMPLETED TASKS:\")\\nprint(f\"   1. ✅ Loaded dataset from Student_At_Risk_Student_Data.csv\")\\nprint(f\"   2. ✅ Verified \\'risk\\' outcome variable exists\")\\nprint(f\"   3. ✅ Reorganized columns: \\'risk\\' at end, \\'country\\' at 2nd position\")\\nprint(f\"   4. ✅ Converted all text data to lowercase\")\\nprint(f\"   5. ✅ Filtered for \\'medium\\' and \\'high\\' risk students only\")\\nprint(f\"   6. ✅ Saved processed data as \\'engineered_student_data.csv\\'\")\\nprint(f\"   7. ✅ Explored data patterns for synthetic generation preparation\")\\n\\nprint(f\"\\n📊 FINAL DATASET STATUS:\")\\nprint(f\"   • File: data/refined_data_for_model/engineered_student_data.csv\")\\nprint(f\"   • Shape: {df.shape}\")\\nprint(f\"   • Risk levels: {list(df[\\'risk\\'].unique())}\")\\nprint(f\"   • Ready for Phase 2 feature engineering\")\\n\\nprint(f\"\\n🚀 NEXT STEPS (Phase 2):\")\\nprint(f\"   • Correlation analysis with outcome variable\")\\nprint(f\"   • Create JSON mapping for categorical variables\")\\nprint(f\"   • Assign numeric weights based on correlations\")\\nprint(f\"   • Sentiment analysis for comments\")\\nprint(f\"   • NLP processing for text features\") '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1 COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✅ COMPLETED TASKS:\")\n",
    "print(f\"   1. ✅ Loaded dataset from Student_At_Risk_Student_Data.csv\")\n",
    "print(f\"   2. ✅ Verified 'risk' outcome variable exists\")\n",
    "print(f\"   3. ✅ Reorganized columns: 'risk' at end, 'country' at 2nd position\")\n",
    "print(f\"   4. ✅ Converted all text data to lowercase\")\n",
    "print(f\"   5. ✅ Filtered for 'medium' and 'high' risk students only\")\n",
    "print(f\"   6. ✅ Saved processed data as 'engineered_student_data.csv'\")\n",
    "print(f\"   7. ✅ Explored data patterns for synthetic generation preparation\")\n",
    "\n",
    "print(f\"\\n📊 FINAL DATASET STATUS:\")\n",
    "print(f\"   • File: data/refined_data_for_model/engineered_student_data.csv\")\n",
    "print(f\"   • Shape: {df.shape}\")\n",
    "print(f\"   • Risk levels: {list(df['risk'].unique())}\")\n",
    "print(f\"   • Ready for Phase 2 feature engineering\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS (Phase 2):\")\n",
    "print(f\"   • Correlation analysis with outcome variable\")\n",
    "print(f\"   • Create JSON mapping for categorical variables\")\n",
    "print(f\"   • Assign numeric weights based on correlations\")\n",
    "print(f\"   • Sentiment analysis for comments\")\n",
    "print(f\"   • NLP processing for text features\") \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
