{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Risk Prediction Model - Week 5 Classification\n",
    "\n",
    "**Objective**: Build a binary classification model to predict student risk level (high vs medium) by Week 5\n",
    "\n",
    "## Project Overview\n",
    "- **Target Variable**: `risk` (binary: high=1, medium=0)\n",
    "- **Timeline**: Week 5 prediction capability  \n",
    "- **Dataset**: 282 students with 41+ features (majority categorical)\n",
    "- **Expected Distribution**: 95 high-risk, 187 medium-risk students\n",
    "- **Success Metric**: High recall for high-risk students (minimize false negatives)\n",
    "\n",
    "## Key Resources\n",
    "- **Input Data**: `data/refined_data_for_model/fully_engineered_student_data.csv`\n",
    "- **Feature Mappings**: `project_info/info_for_predictive_model/feature_mappings.json`\n",
    "- **Implementation Guide**: `project_info/info_for_predictive_model/predictive_model_steps.md`\n",
    "\n",
    "This notebook implements the 11-step process defined in CLAUDE.md for systematic model development.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Objective Definition ‚úÖ\n",
    "\n",
    "### Classification Task Definition\n",
    "- **Problem Type**: Binary Classification\n",
    "- **Target Variable**: `risk` with values ['high', 'medium'] \n",
    "- **Encoding**: medium=0, high=1\n",
    "- **Business Goal**: Identify students at high risk by Week 5 to enable early intervention\n",
    "\n",
    "### Success Criteria\n",
    "1. **Primary**: High recall for high-risk students (minimize missed high-risk cases)\n",
    "2. **Secondary**: Overall F1-score and accuracy\n",
    "3. **Tertiary**: Model interpretability for educational stakeholders\n",
    "\n",
    "### Dataset Specifications\n",
    "- **Size**: 282 students, 41+ features\n",
    "- **Class Distribution**: 95 high-risk (33.7%), 187 medium-risk (66.3%)\n",
    "- **Feature Types**: Mixed (numeric assessments, categorical demographics, engineered weights)\n",
    "- **Data Source**: Fully engineered dataset with risk-based feature weights\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import required libraries for the prediction model\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning imports\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score, \n    precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n)\n\n# For XGBoost (if available)\ntry:\n    import xgboost as xgb\n    XGBOOST_AVAILABLE = True\n    print(\"XGBoost available\")\nexcept ImportError:\n    XGBOOST_AVAILABLE = False\n    print(\"XGBoost not available - will use alternative models\")\n\n# For Jupyter notebook display\ntry:\n    from IPython.display import display\n    DISPLAY_AVAILABLE = True\nexcept ImportError:\n    DISPLAY_AVAILABLE = False\n    # Fallback display function\n    def display(obj):\n        print(obj)\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"Libraries imported successfully!\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Matplotlib version: {plt.matplotlib.__version__}\")\nprint(f\"Seaborn version: {sns.__version__}\")\nprint(\"Scikit-learn imported\")\n\n# Define constants for our binary classification task\nTARGET_COLUMN = 'risk'\nHIGH_RISK_LABEL = 'high'\nMEDIUM_RISK_LABEL = 'medium'\nBINARY_ENCODING = {MEDIUM_RISK_LABEL: 0, HIGH_RISK_LABEL: 1}\n\nprint(f\"\\nüìã PROJECT SETUP:\")\nprint(f\"Target variable: {TARGET_COLUMN}\")\nprint(f\"Binary encoding: {BINARY_ENCODING}\")\nprint(f\"Focus: Minimize false negatives for {HIGH_RISK_LABEL}-risk students\")"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: DATA OVERVIEW & VALIDATION\n",
      "============================================================\n",
      "Loading data from: ../data/refined_data_for_model/fully_engineered_student_data.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df = \u001b[43mpd\u001b[49m.read_csv(data_path)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Data loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2: Load and validate the fully engineered dataset\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: DATA OVERVIEW & VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the fully engineered dataset\n",
    "data_path = '../data/refined_data_for_model/fully_engineered_student_data.csv'\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Error: fully_engineered_student_data.csv not found!\")\n",
    "    print(\"Available files:\")\n",
    "    import os\n",
    "    data_dir = '../data/refined_data_for_model/'\n",
    "    if os.path.exists(data_dir):\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(\"  - Data directory not found\")\n",
    "\n",
    "    # Try alternative files\n",
    "    alt_files = [\n",
    "        '../data/refined_data_for_model/engineered_student_data.csv',\n",
    "        '../data/refined_data_for_model/Student_At_Risk_Student_Data.csv'\n",
    "    ]\n",
    "\n",
    "    df = None\n",
    "    for alt_file in alt_files:\n",
    "        try:\n",
    "            df = pd.read_csv(alt_file)\n",
    "            print(f\"‚úÖ Using alternative file: {alt_file}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    if df is None:\n",
    "        print(\"‚ùå No suitable data file found. Please check file paths.\")\n",
    "        raise FileNotFoundError(\"Required data file not found\")\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Validate expected dimensions\n",
    "expected_rows = 282\n",
    "expected_cols_min = 41\n",
    "\n",
    "print(f\"\\nüîç VALIDATION CHECKS:\")\n",
    "print(f\"Expected rows: {expected_rows}, Actual: {df.shape[0]} {'‚úÖ' if df.shape[0] == expected_rows else '‚ö†Ô∏è'}\")\n",
    "print(f\"Expected columns: {expected_cols_min}+, Actual: {df.shape[1]} {'‚úÖ' if df.shape[1] >= expected_cols_min else '‚ö†Ô∏è'}\")\n",
    "\n",
    "# Check for target variable\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    print(f\"Target variable '{TARGET_COLUMN}': ‚úÖ Found\")\n",
    "    target_values = df[TARGET_COLUMN].unique()\n",
    "    print(f\"Target values: {target_values}\")\n",
    "\n",
    "    # Check expected risk levels\n",
    "    expected_values = [HIGH_RISK_LABEL, MEDIUM_RISK_LABEL]\n",
    "    has_expected = all(val in target_values for val in expected_values)\n",
    "    print(f\"Expected risk levels {expected_values}: {'‚úÖ' if has_expected else '‚ö†Ô∏è'}\")\n",
    "else:\n",
    "    print(f\"Target variable '{TARGET_COLUMN}': ‚ùå Not found\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "\n",
    "    # Look for similar columns\n",
    "    risk_cols = [col for col in df.columns if 'risk' in col.lower()]\n",
    "    if risk_cols:\n",
    "        print(f\"Possible risk columns: {risk_cols}\")\n",
    "\n",
    "print(f\"\\nüìã COLUMN SUMMARY:\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed data analysis and validation\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED DATA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Risk distribution analysis\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    print(f\"\\nüéØ TARGET VARIABLE ANALYSIS:\")\n",
    "    risk_counts = df[TARGET_COLUMN].value_counts()\n",
    "    risk_percentages = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "\n",
    "    print(f\"Risk Distribution:\")\n",
    "    for risk_level in risk_counts.index:\n",
    "        count = risk_counts[risk_level]\n",
    "        percentage = risk_percentages[risk_level]\n",
    "        print(f\"  {risk_level}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "    # Validate expected distribution\n",
    "    if HIGH_RISK_LABEL in risk_counts and MEDIUM_RISK_LABEL in risk_counts:\n",
    "        high_count = risk_counts[HIGH_RISK_LABEL]\n",
    "        medium_count = risk_counts[MEDIUM_RISK_LABEL]\n",
    "\n",
    "        print(f\"\\nüìä DISTRIBUTION VALIDATION:\")\n",
    "        print(f\"Expected high-risk: ~95, Actual: {high_count} {'‚úÖ' if 85 <= high_count <= 105 else '‚ö†Ô∏è'}\")\n",
    "        print(f\"Expected medium-risk: ~187, Actual: {medium_count} {'‚úÖ' if 177 <= medium_count <= 197 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "        # Class imbalance ratio\n",
    "        total = high_count + medium_count\n",
    "        imbalance_ratio = medium_count / high_count if high_count > 0 else 0\n",
    "        print(f\"Class imbalance ratio (medium:high): {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "        if imbalance_ratio > 1.5:\n",
    "            print(\"‚ö†Ô∏è  Imbalanced dataset detected - will need class weighting\")\n",
    "        else:\n",
    "            print(\"‚úÖ Reasonably balanced dataset\")\n",
    "\n",
    "# Feature type analysis\n",
    "print(f\"\\nüîç FEATURE TYPE ANALYSIS:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns from feature lists\n",
    "if TARGET_COLUMN in numeric_cols:\n",
    "    numeric_cols.remove(TARGET_COLUMN)\n",
    "if TARGET_COLUMN in categorical_cols:\n",
    "    categorical_cols.remove(TARGET_COLUMN)\n",
    "\n",
    "id_cols = [col for col in df.columns if 'id' in col.lower()]\n",
    "numeric_cols = [col for col in numeric_cols if col not in id_cols]\n",
    "categorical_cols = [col for col in categorical_cols if col not in id_cols]\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"ID columns: {len(id_cols)} {id_cols}\")\n",
    "\n",
    "# Check for engineered features\n",
    "weighted_cols = [col for col in df.columns if col.endswith('_weighted')]\n",
    "sentiment_cols = [col for col in df.columns if 'sentiment' in col.lower()]\n",
    "\n",
    "print(f\"\\nüîß ENGINEERED FEATURES:\")\n",
    "print(f\"Weighted categorical features: {len(weighted_cols)}\")\n",
    "if len(weighted_cols) > 0:\n",
    "    print(f\"  Examples: {weighted_cols[:3]}\")\n",
    "print(f\"Sentiment features: {len(sentiment_cols)}\")\n",
    "if len(sentiment_cols) > 0:\n",
    "    print(f\"  Examples: {sentiment_cols[:3]}\")\n",
    "\n",
    "# Missing data analysis\n",
    "print(f\"\\nüîç MISSING DATA ANALYSIS:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(f\"Columns with missing data: {len(missing_data)}\")\n",
    "    print(\"Top missing data columns:\")\n",
    "    for col, count in missing_data.head(10).items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {col}: {count} missing ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing data found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Step 2 validation completed!\")\n",
    "print(f\"Dataset ready for preprocessing: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key data insights\n",
    "print(\"=\"*60)\n",
    "print(\"DATA VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Risk distribution\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    risk_counts = df[TARGET_COLUMN].value_counts()\n",
    "    axes[0, 0].pie(risk_counts.values, labels=risk_counts.index, autopct='%1.1f%%',\n",
    "                   colors=['#ff7f7f', '#87ceeb'])\n",
    "    axes[0, 0].set_title('Risk Level Distribution')\n",
    "\n",
    "# 2. Feature types breakdown\n",
    "feature_types = {\n",
    "    'Numeric': len(numeric_cols),\n",
    "    'Categorical': len(categorical_cols),\n",
    "    'Weighted': len(weighted_cols),\n",
    "    'Sentiment': len(sentiment_cols)\n",
    "}\n",
    "\n",
    "axes[0, 1].bar(feature_types.keys(), feature_types.values(), color=['skyblue', 'lightgreen', 'gold', 'pink'])\n",
    "axes[0, 1].set_title('Feature Types Overview')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# 3. Missing data visualization (if any)\n",
    "if len(missing_data) > 0:\n",
    "    top_missing = missing_data.head(10)\n",
    "    axes[1, 0].barh(range(len(top_missing)), top_missing.values)\n",
    "    axes[1, 0].set_yticks(range(len(top_missing)))\n",
    "    axes[1, 0].set_yticklabels(top_missing.index)\n",
    "    axes[1, 0].set_title('Top 10 Columns with Missing Data')\n",
    "    axes[1, 0].set_xlabel('Missing Count')\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No Missing Data ‚úÖ', ha='center', va='center',\n",
    "                    transform=axes[1, 0].transAxes, fontsize=16)\n",
    "    axes[1, 0].set_title('Missing Data Status')\n",
    "\n",
    "# 4. Dataset size visualization\n",
    "size_info = {\n",
    "    'Total Rows': df.shape[0],\n",
    "    'Total Columns': df.shape[1],\n",
    "    'Expected Rows': 282,\n",
    "    'Expected Cols': 41\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(size_info))\n",
    "values = list(size_info.values())\n",
    "axes[1, 1].bar(x_pos, values, color=['lightcoral', 'lightblue', 'lightcoral', 'lightblue'], alpha=0.7)\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(size_info.keys(), rotation=45)\n",
    "axes[1, 1].set_title('Dataset Size Validation')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìã SAMPLE DATA:\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display_cols = df.columns[:10].tolist()  # Show first 10 columns\n",
    "print(df[display_cols].head())\n",
    "\n",
    "# Show data types\n",
    "print(f\"\\nüìä DATA TYPES SUMMARY:\")\n",
    "dtype_summary = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_summary.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "print(f\"\\nüéØ Step 2 Complete: Data loaded and validated successfully!\")\n",
    "print(f\"Ready to proceed to Step 3: Data Preprocessing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}