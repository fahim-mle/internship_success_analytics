{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model for Student Risk Assessment\n",
    "\n",
    "This notebook focuses on building machine learning models to predict students at high and medium risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with engineered features\n",
    "df = pd.read_csv('../data/refined_data_for_model/Student_Data_With_Features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable\n",
    "target_cols = [col for col in df.columns if any(x in col.lower() for x in ['risk', 'target', 'label', 'outcome'])]\n",
    "print(f\"Potential target columns: {target_cols}\")\n",
    "\n",
    "# If no clear target exists, create one based on available features\n",
    "if not target_cols:\n",
    "    # Check for academic status or performance indicators\n",
    "    if 'avg_assessment' in df.columns:\n",
    "        df['risk_level'] = pd.cut(df['avg_assessment'], \n",
    "                                 bins=3, \n",
    "                                 labels=['high_risk', 'medium_risk', 'low_risk'])\n",
    "        target_col = 'risk_level'\n",
    "    elif any('academic_status' in col.lower() for col in df.columns):\n",
    "        academic_col = [col for col in df.columns if 'academic_status' in col.lower()][0]\n",
    "        target_col = academic_col\n",
    "    else:\n",
    "        print(\"No suitable target variable found. Please specify the target column.\")\n",
    "        target_col = None\n",
    "else:\n",
    "    target_col = target_cols[0]\n",
    "\n",
    "print(f\"Using target variable: {target_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "if target_col and target_col in df.columns:\n",
    "    print(f\"Target variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df[target_col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {target_col}')\n",
    "    plt.xlabel('Risk Level')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "if target_col and target_col in df.columns:\n",
    "    # Remove non-predictive columns\n",
    "    drop_cols = ['student_id'] + [col for col in df.columns if 'id' in col.lower()]\n",
    "    drop_cols = [col for col in drop_cols if col in df.columns]\n",
    "    \n",
    "    # Feature matrix\n",
    "    X = df.drop(columns=[target_col] + drop_cols)\n",
    "    \n",
    "    # Target variable\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Encode target if categorical\n",
    "    if y.dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        y_encoded = le.fit_transform(y)\n",
    "        print(f\"Target encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "    else:\n",
    "        y_encoded = y\n",
    "        le = None\n",
    "    \n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Features: {list(X.columns)}\")\n",
    "else:\n",
    "    print(\"Cannot proceed without a valid target variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "if 'X' in locals() and 'y_encoded' in locals():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
    "    print(f\"Test target distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Use scaled data for SVM and Logistic Regression\n",
    "    if name in ['SVM', 'Logistic Regression']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_model, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'Precision': [results[model]['precision'] for model in results.keys()],\n",
    "    'Recall': [results[model]['recall'] for model in results.keys()],\n",
    "    'F1-Score': [results[model]['f1_score'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    comparison_df.plot(x='Model', y=metric, kind='bar', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.legend().remove()\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        importance = model.feature_importances_\n",
    "        \n",
    "        # Create feature importance dataframe\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{model_name} - Top 10 Important Features:\")\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_features = feature_importance_df.head(10)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'{model_name} - Feature Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Model Selection and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1-score\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['f1_score'])\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"F1-Score: {results[best_model_name]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for best model\n",
    "if best_model_name == 'Random Forest':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    base_model = RandomForestClassifier(random_state=42)\n",
    "    X_tune = X_train\n",
    "    \n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.1, 0.2],\n",
    "        'max_depth': [3, 5]\n",
    "    }\n",
    "    base_model = GradientBoostingClassifier(random_state=42)\n",
    "    X_tune = X_train\n",
    "    \n",
    "elif best_model_name == 'Logistic Regression':\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "    base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    X_tune = X_train_scaled\n",
    "    \n",
    "else:  # SVM\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    base_model = SVC(random_state=42, probability=True)\n",
    "    X_tune = X_train_scaled\n",
    "\n",
    "# Grid search\n",
    "print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "grid_search = GridSearchCV(base_model, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_tune, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on test set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Use appropriate test data\n",
    "if best_model_name in ['SVM', 'Logistic Regression']:\n",
    "    X_test_final = X_test_scaled\n",
    "else:\n",
    "    X_test_final = X_test\n",
    "\n",
    "# Final predictions\n",
    "y_pred_final = best_model.predict(X_test_final)\n",
    "y_pred_proba_final = best_model.predict_proba(X_test_final)\n",
    "\n",
    "# Final metrics\n",
    "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "final_precision = precision_score(y_test, y_pred_final, average='weighted')\n",
    "final_recall = recall_score(y_test, y_pred_final, average='weighted')\n",
    "final_f1 = f1_score(y_test, y_pred_final, average='weighted')\n",
    "\n",
    "print(f\"\\nFinal {best_model_name} Performance:\")\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Risk Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions by risk level\n",
    "if le is not None:\n",
    "    # Convert predictions back to original labels\n",
    "    y_test_labels = le.inverse_transform(y_test)\n",
    "    y_pred_labels = le.inverse_transform(y_pred_final)\n",
    "    \n",
    "    # Create prediction summary\n",
    "    prediction_summary = pd.DataFrame({\n",
    "        'Actual': y_test_labels,\n",
    "        'Predicted': y_pred_labels\n",
    "    })\n",
    "    \n",
    "    print(\"Prediction Summary:\")\n",
    "    print(pd.crosstab(prediction_summary['Actual'], prediction_summary['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Final Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "if le is not None:\n",
    "    plt.xticks(range(len(le.classes_)), le.classes_)\n",
    "    plt.yticks(range(len(le.classes_)), le.classes_)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   Final F1-Score: {final_f1:.4f}\")\n",
    "print(f\"   Final Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\n2. MODEL PARAMETERS:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    print(f\"\\n3. TOP RISK INDICATORS:\")\n",
    "    importance = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(5).iterrows()):\n",
    "        print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. RECOMMENDATIONS:\")\n",
    "print(f\"   - Monitor students with high-risk predictions closely\")\n",
    "print(f\"   - Implement early intervention programs\")\n",
    "print(f\"   - Focus on top risk indicators for prevention strategies\")\n",
    "print(f\"   - Regularly retrain model with new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Model Deployment**: Prepare the model for production use\n",
    "2. **Monitoring**: Set up model performance monitoring\n",
    "3. **Intervention Design**: Create targeted intervention strategies based on risk factors\n",
    "4. **Data Collection**: Continuously collect new data for model retraining\n",
    "5. **Feature Engineering**: Explore additional features that might improve prediction accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}